{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00514/Bias_correction_ucl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "station              2\n",
       "Date                 2\n",
       "Present_Tmax        70\n",
       "Present_Tmin        70\n",
       "LDAPS_RHmin         75\n",
       "LDAPS_RHmax         75\n",
       "LDAPS_Tmax_lapse    75\n",
       "LDAPS_Tmin_lapse    75\n",
       "LDAPS_WS            75\n",
       "LDAPS_LH            75\n",
       "LDAPS_CC1           75\n",
       "LDAPS_CC2           75\n",
       "LDAPS_CC3           75\n",
       "LDAPS_CC4           75\n",
       "LDAPS_PPT1          75\n",
       "LDAPS_PPT2          75\n",
       "LDAPS_PPT3          75\n",
       "LDAPS_PPT4          75\n",
       "lat                  0\n",
       "lon                  0\n",
       "DEM                  0\n",
       "Slope                0\n",
       "Solar radiation      0\n",
       "Next_Tmax           27\n",
       "Next_Tmin           27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df: DataFrame) -> DataFrame:\n",
    "    df_processed = df.copy()\n",
    "    df_processed = df_processed.dropna(subset=['Next_Tmax', 'Date'])\n",
    "    df_processed.reset_index(drop=True, inplace=True)\n",
    "    # missing_cols = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    df_date = df_processed['Date']\n",
    "    df_processed = df_processed.drop(['Date'], axis=1)\n",
    "    # df_missing = df[['enrollee_id'] + missing_cols]\n",
    "    # df_non_missing = df.drop(missing_cols, axis=1)\n",
    "\n",
    "    knn_imputer = KNNImputer(n_neighbors=1)\n",
    "\n",
    "    X = knn_imputer.fit_transform(df_processed)\n",
    "\n",
    "    df_processed = pd.DataFrame(X, columns = df_processed.columns)\n",
    "\n",
    "    df_processed['Date'] = df_date\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = handle_missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "- Extract some informations from Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month\n",
    "df_processed['month'] = df_processed.Date.dt.month\n",
    "\n",
    "# Day of month\n",
    "df_processed['day_of_month'] = df_processed.Date.dt.day\n",
    "\n",
    "# Day of week\n",
    "df_processed['day_of_week'] = df_processed.Date.dt.day_name()\n",
    "\n",
    "# Year: for data split purpose only\n",
    "df_processed['year'] = df_processed.Date.dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "- Remove features with low correlation with Next_Tmax: |corr| < 0.15\n",
    "- Remove Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split categoric and numeric data\n",
    "numeric_data = list(set(df_processed.columns) - set(['station', 'month', 'day_of_month', 'day_of_week', 'year']))\n",
    "categoric_data = ['station', 'month', 'day_of_month', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lat',\n",
       " 'LDAPS_PPT1',\n",
       " 'Solar radiation',\n",
       " 'lon',\n",
       " 'Slope',\n",
       " 'Date',\n",
       " 'Next_Tmin',\n",
       " 'Next_Tmax']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify columns with low correlation\n",
    "corrMatrix = df_processed[numeric_data].corr()\n",
    "columns_to_drop = list(corrMatrix[(np.absolute(corrMatrix['Next_Tmax']) < 0.15)]['Next_Tmax'].index)\n",
    "columns_to_drop = columns_to_drop + ['Date', 'Next_Tmin', 'Next_Tmax']\n",
    "columns_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop(columns_to_drop, axis = 1)\n",
    "y = df_processed['Next_Tmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = list(set(numeric_data) - set(columns_to_drop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnualTimeSeriesSplit():\n",
    "    def __init__(self, first_year, n_years):\n",
    "        self.n_splits = n_years\n",
    "        self.first_year = first_year\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        for i in range(self.n_splits):\n",
    "            start = 0\n",
    "            mid = X[X['year'] == self.first_year + i].index[-1] + 1\n",
    "            stop = X[X['year'] == self.first_year + i + 1].index[-1] + 1\n",
    "            yield indices[start: mid], indices[mid: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6185"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SPLIT_ID = int(len(y)*0.7)\n",
    "# set SPLIT_ID by the end of the first 4 years on the dataset\n",
    "SPLIT_ID = X[X['year'] != 2017].index[-1] + 1\n",
    "SPLIT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.iloc[:SPLIT_ID]\n",
    "y_train = y.iloc[:SPLIT_ID]\n",
    "\n",
    "X_test = X.iloc[SPLIT_ID:]\n",
    "y_test = y.iloc[SPLIT_ID:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=4)\n",
    "atscv = AnualTimeSeriesSplit(first_year=2013, n_years=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model=LGBMRegressor()):\n",
    "    transformers=[('cat_scale', OneHotEncoder(), categoric_data),\n",
    "                ('num_scale', MinMaxScaler(), numeric_data)]\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers)\n",
    "\n",
    "    steps = [('preprocessor', preprocessor),\n",
    "            ('model', model)]\n",
    "\n",
    "    model_pipeline = Pipeline(steps=steps, verbose=False)\n",
    "    return model_pipeline\n",
    "\n",
    "lgb_model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lgb_model, X_train, y_train, cv=atscv, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.698 (+/- 0.104)\n"
     ]
    }
   ],
   "source": [
    "print(\"R2 score: {0:.3f} (+/- {1:.3f})\".format(scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridParams = {\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__n_estimators': [24,50,75,100],\n",
    "    'model__num_leaves': [8,12,16,31,48,72], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'model__random_state' : [42],\n",
    "    'model__subsample' : [0.5, 0.7, 1],\n",
    "    'model__reg_alpha' : [1,1.2],\n",
    "    'model__reg_lambda' : [1,1.2,1.4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "gs = GridSearchCV(model, param_grid=gridParams, cv=atscv, scoring='r2', return_train_score=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=<__main__.AnualTimeSeriesSplit object at 0x000001F0E5CCC070>,\n",
       "             estimator=Pipeline(steps=[('preprocessor',\n",
       "                                        ColumnTransformer(transformers=[('cat_scale',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         ['station',\n",
       "                                                                          'month',\n",
       "                                                                          'day_of_month',\n",
       "                                                                          'day_of_week']),\n",
       "                                                                        ('num_scale',\n",
       "                                                                         MinMaxScaler(),\n",
       "                                                                         ['LDAPS_RHmax',\n",
       "                                                                          'LDAPS_WS',\n",
       "                                                                          'LDAPS_PPT2',\n",
       "                                                                          'LDAPS_PPT4',\n",
       "                                                                          'DEM',\n",
       "                                                                          'LDAPS_LH',\n",
       "                                                                          'LDAPS_PPT3',\n",
       "                                                                          'LDAPS_CC1',\n",
       "                                                                          '...\n",
       "                                                                          'LDAPS_Tmin_lapse',\n",
       "                                                                          'LDAPS_Tmax_lapse',\n",
       "                                                                          'LDAPS_CC3'])])),\n",
       "                                       ('lgb', LGBMRegressor())]),\n",
       "             param_grid={'lgb__learning_rate': [0.01, 0.1],\n",
       "                         'lgb__n_estimators': [24, 50, 75, 100],\n",
       "                         'lgb__num_leaves': [8, 12, 16, 31, 48, 72],\n",
       "                         'lgb__random_state': [42], 'lgb__reg_alpha': [1, 1.2],\n",
       "                         'lgb__reg_lambda': [1, 1.2, 1.4],\n",
       "                         'lgb__subsample': [0.5, 0.7, 1]},\n",
       "             return_train_score=True, scoring='r2', verbose=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(transformers=[('cat_scale', OneHotEncoder(),\n",
       "                                                  ['station', 'month',\n",
       "                                                   'day_of_month',\n",
       "                                                   'day_of_week']),\n",
       "                                                 ('num_scale', MinMaxScaler(),\n",
       "                                                  ['LDAPS_RHmax', 'LDAPS_WS',\n",
       "                                                   'LDAPS_PPT2', 'LDAPS_PPT4',\n",
       "                                                   'DEM', 'LDAPS_LH',\n",
       "                                                   'LDAPS_PPT3', 'LDAPS_CC1',\n",
       "                                                   'LDAPS_CC2', 'LDAPS_CC4',\n",
       "                                                   'LDAPS_RHmin',\n",
       "                                                   'Present_Tmax',\n",
       "                                                   'Present_Tmin',\n",
       "                                                   'LDAPS_Tmin_lapse',\n",
       "                                                   'LDAPS_Tmax_lapse',\n",
       "                                                   'LDAPS_CC3'])])),\n",
       "                ('lgb',\n",
       "                 LGBMRegressor(random_state=42, reg_alpha=1.2, reg_lambda=1,\n",
       "                               subsample=0.5))])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2 Score: 0.663\n",
      "Test RMSE Score: 1.828\n",
      "Test MAE Score: 1.333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(LGBMRegressor(random_state=42, reg_alpha=1.2, reg_lambda=1,\n",
    "                               subsample=0.5))\n",
    "# train model\n",
    "model.fit(X_train, y_train)\n",
    "# test score\n",
    "y_predicted = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_predicted, multioutput='uniform_average')\n",
    "rmse = mean_squared_error(y_test, y_predicted, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_predicted)\n",
    "print(\"Test R2 Score: {0:.3f}\".format(r2))\n",
    "print(\"Test RMSE Score: {0:.3f}\".format(rmse))\n",
    "print(\"Test MAE Score: {0:.3f}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPReg(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b9a9829bfd4bd22d5b3b20a990b98b71f85420816dd0dfc2e76761f41c6bb45"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('pos': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
